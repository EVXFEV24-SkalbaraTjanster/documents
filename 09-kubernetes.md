# Kubernetes - Container Orchestration i Praktiken

## Introduktion

Docker Compose Ã¤r som att dirigera en liten orkester i ditt vardagsrum. Kubernetes Ã¤r som att dirigera en symfoniorkester pÃ¥ en stor konsertsal - med hundratals musiker, flera dirigenter, och publik frÃ¥n hela vÃ¤rlden.

TÃ¤nk dig att du har 100 containers - ska du verkligen starta, stoppa och Ã¶vervaka alla manuellt? Och vad hÃ¤nder nÃ¤r en container kraschar klockan 03:00 pÃ¥ natten? Ska du vakna och starta om den? Absolut inte!

Det Ã¤r hÃ¤r **Kubernetes** (ofta fÃ¶rkortat **K8s**) kommer in i bilden. Det Ã¤r plattformen som gÃ¶r det mÃ¶jligt att kÃ¶ra containers i produktion pÃ¥ riktigt stort - med automatisk skalning, self-healing, och allt det dÃ¤r som gÃ¶r att du faktiskt kan sova pÃ¥ nÃ¤tterna.

### Vad Ã¤r Kubernetes?

Kubernetes Ã¤r ett **container orchestration system** - ett system som automatiskt hanterar, schemalÃ¤ggerar och Ã¶vervakar containers Ã¶ver flera servrar. Det utvecklades ursprungligen av Google baserat pÃ¥ deras interna system "Borg" som de anvÃ¤nt i Ã¶ver 15 Ã¥r fÃ¶r att hantera miljontals containers.

Idag Ã¤r Kubernetes open source och har blivit industristandard fÃ¶r container orchestration. Det Ã¤r det system som driver Netflix, Spotify, Airbnb, och tusentals andra fÃ¶retag som kÃ¶r containers i produktion.

## VarfÃ¶r Kubernetes?

### Problemet

NÃ¤r din applikation vÃ¤xer stÃ¶ter du pÃ¥ problem som Docker Compose inte kan lÃ¶sa:

- **Flera servrar**: Compose kÃ¶rs pÃ¥ en enda maskin. Vad hÃ¤nder nÃ¤r du behÃ¶ver 10, 20, eller 100 servrar?
- **Ingen auto-scaling**: Du mÃ¥ste manuellt Ã¤ndra antal containers
- **Ingen self-healing**: Om en container kraschar mÃ¥ste du starta om den manuellt
- **Manual deployment**: Du mÃ¥ste SSH:a in pÃ¥ varje server och uppdatera
- **Ingen load balancing**: Du mÃ¥ste sjÃ¤lv sÃ¤tta upp nginx eller liknande
- **Inget automatiskt failover**: Om en server dÃ¶r, vad hÃ¤nder med containers som kÃ¶rde dÃ¤r?

### LÃ¶sningen - Vad Kubernetes Ger Dig

Kubernetes lÃ¶ser alla dessa problem och mer dÃ¤rtill:

**1. Multi-host deployment**

- KÃ¶r containers Ã¶ver mÃ¥nga servrar automatiskt
- Du bryr dig inte om vilken server - K8s bestÃ¤mmer

**2. Auto-scaling**

- LÃ¤gg till fler containers nÃ¤r lasten Ã¶kar
- Ta bort containers nÃ¤r lasten minskar
- Spara pengar automatiskt

**3. Self-healing**

- Container kraschar? K8s startar en ny automatiskt
- Server dÃ¶r? K8s flyttar containers till andra servrar
- Health checks Ã¶vervakar allt

**4. Rolling updates**

- Uppdatera din app utan downtime
- Gradvis utrullning med automatisk rollback vid problem

**5. Built-in load balancing**

- FÃ¶rdela trafik mellan containers automatiskt
- Ingen extra nginx behÃ¶vs

**6. Service discovery**

- Services hittar varandra automatiskt
- Inget hÃ¥rdkodat IPs eller DNS

**7. Secret management**

- Hantera kÃ¤nsliga data sÃ¤kert
- API-nycklar, lÃ¶senord, certifikat

**8. Resource management**

- FÃ¶rdela CPU och minne effektivt
- FÃ¶rhindra att en app tar alla resurser

### Verkligt Scenario

SÃ¤g att du har en e-handelsplattform med 50 microservices:

- Varje service behÃ¶ver 3-10 instanser
- Det blir 200+ containers
- Som kÃ¶rs pÃ¥ 20 servrar
- Och ska vara tillgÃ¤ngliga 24/7

**Utan Kubernetes:**

- Manuell deployment pÃ¥ varje server
- Manuell Ã¶vervakning av 200+ containers
- Manuell skalning vid trafikÃ¶kningar
- Panik nÃ¤r nÃ¥got gÃ¥r sÃ¶nder klockan 03:00
- = OmÃ¶jligt att hantera

**Med Kubernetes:**

- Deploy med ett kommando
- Automatisk Ã¶vervakning och restart
- Automatisk skalning baserat pÃ¥ last
- Self-healing - du sover gott
- = Hanterbart och skalbart

## Docker Compose vs Kubernetes

LÃ¥t oss gÃ¶ra en tydlig jÃ¤mfÃ¶relse:

### Docker Compose

**AnvÃ¤ndningsomrÃ¥de:**

- Utveckling och testmiljÃ¶er
- SmÃ¥ applikationer
- Single-host (en server)
- Personliga projekt
- Prototyper

**FÃ¶rdelar:**

- Super enkelt att komma igÃ¥ng
- LÃ¤sbar YAML-fil
- Perfekt fÃ¶r utveckling
- Ingen komplex infrastruktur

**BegrÃ¤nsningar:**

- Endast en server
- Ingen auto-scaling
- Ingen self-healing
- Ingen high availability
- Manual updates

**Exempel:**

```yaml
version: "3"
services:
  app:
    image: myapp:1.0
    ports:
      - "8080:8080"
  db:
    image: postgres:15
```

Simple! Men vad hÃ¤nder om servern gÃ¥r ner? Hela din app Ã¤r nere.

### Kubernetes

**AnvÃ¤ndningsomrÃ¥de:**

- Produktion
- Flera servrar (cluster)
- Stora applikationer
- Enterprise
- Microservices

**FÃ¶rdelar:**

- Multi-host clustering
- Automatisk skalning
- Self-healing
- High availability
- Rolling updates
- Load balancing built-in
- AnvÃ¤nds av alla stora tech-fÃ¶retag

**Utmaningar:**

- StÃ¶rre learning curve
- Mer komplex setup
- BehÃ¶ver mer infrastruktur
- Overkill fÃ¶r smÃ¥ projekt

**NÃ¤r ska du vÃ¤lja vad?**

```
Projekt storlek / komplexitet
    |
    |                    K8s-zonen
    |              ___________________
    |             |                   |
    |             |  Microservices    |
    |             |  Produktion       |
    |             |  Auto-scaling     |
    |   Compose   |___________________| 
    |   zonen     
    |_________
    |         |
    |  Dev    |
    |  Test   |
    |  SmÃ¥    |
    |_________|
    |
    +---------------------------------> Tid/Resurser
```

**Tumregel:** BÃ¶rja med Docker Compose. NÃ¤r du behÃ¶ver flera servrar, auto-scaling, eller har ett stÃ¶rre team - dÃ¥ Ã¤r det dags fÃ¶r Kubernetes.

## Kubernetes Arkitektur (Ã–versikt)

Kubernetes kÃ¶rs som ett **cluster** - en samling maskiner som jobbar tillsammans. Vissa maskiner Ã¤r "hjÃ¤rnan" (control plane) och andra Ã¤r "arbetare" (worker nodes).

```
                KUBERNETES CLUSTER

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           CONTROL PLANE (Master)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  API Server  â”‚  Scheduler  â”‚  Controller â”‚  â”‚
â”‚  â”‚              â”‚             â”‚   Manager   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              etcd (cluster data)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚ NODE 1 â”‚   â”‚ NODE 2 â”‚   â”‚ NODE 3 â”‚
â”‚        â”‚   â”‚        â”‚   â”‚        â”‚
â”‚ Pod    â”‚   â”‚ Pod    â”‚   â”‚ Pod    â”‚
â”‚ Pod    â”‚   â”‚ Pod    â”‚   â”‚ Pod    â”‚
â”‚ Pod    â”‚   â”‚        â”‚   â”‚ Pod    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Worker       Worker       Worker
```

### Control Plane (HjÃ¤rnan)

Control plane Ã¤r "management layer" som bestÃ¤mmer vad som ska hÃ¤nda i clustret:

**API Server**

- Entry point fÃ¶r allt
- NÃ¤r du kÃ¶r `kubectl`-kommandon pratar du med API Server
- Alla komponenter kommunicerar genom den

**Scheduler**

- BestÃ¤mmer vilken worker node som ska kÃ¶ra nya pods
- Kollar: vilken node har plats? Vilka resurser behÃ¶vs?
- Smart placering fÃ¶r optimal resursanvÃ¤ndning

**Controller Manager**

- SÃ¤kerstÃ¤ller att Ã¶nskat tillstÃ¥nd upprÃ¤tthÃ¥lls
- "Vi vill ha 3 replicas" - Controller ser till att det alltid Ã¤r 3
- Om en pod dÃ¶r, startar Controller en ny

**etcd**

- Distribuerad databas som lagrar hela cluster state
- Alla konfigurationer, secrets, tillstÃ¥nd
- Kritisk komponent - utan etcd, inget cluster

### Worker Nodes (Arbetarna)

Worker nodes Ã¤r de maskiner som faktiskt kÃ¶r dina containers:

**Kubelet**

- Agent som kÃ¶rs pÃ¥ varje node
- Tar emot instruktioner frÃ¥n control plane
- Startar och Ã¶vervakar containers
- Rapporterar status tillbaka

**Container Runtime**

- Den faktiska motor som kÃ¶r containers
- Oftast containerd eller Docker
- Kubelet pratar med runtime fÃ¶r att starta containers

**Kube-proxy**

- Hanterar networking pÃ¥ noden
- Routing av trafik till rÃ¤tt pods
- Implementerar Services (mer om det senare)

### Hur det fungerar tillsammans

1. Du skickar en request via `kubectl`: "KÃ¶r 3 instanser av min app"
2. API Server tar emot requesten
3. Scheduler bestÃ¤mmer vilka nodes som ska kÃ¶ra pods
4. Kubelet pÃ¥ varje node startar containers
5. Controller Manager Ã¶vervakar och sÃ¤kerstÃ¤ller 3 pods alltid kÃ¶rs
6. etcd sparar allt detta

## GrundlÃ¤ggande Koncept

Nu nÃ¤r vi fÃ¶rstÃ¥r arkitekturen, lÃ¥t oss gÃ¥ igenom de viktigaste koncepten i Kubernetes.

### Pod - Den Minsta Enheten

En **Pod** Ã¤r den minsta deployable enheten i Kubernetes. Den Ã¤r som ett "hus" fÃ¶r en eller flera containers som behÃ¶ver kÃ¶ras tillsammans.

**Viktiga punkter:**

- Vanligtvis en container per pod
- Ibland flera containers som Ã¤r tight coupled (sidecar pattern)
- Pods Ã¤r **ephemeral** - de kan dÃ¶ och skapas nÃ¤r som helst
- Varje pod fÃ¥r sin egen IP-adress
- Containers i samma pod delar nÃ¤tverk och storage

**Analogi:**
TÃ¤nk pÃ¥ en Pod som ett hus. Vanligtvis bor en person (container) i huset, men ibland kan det bo flera som behÃ¶ver vara nÃ¤ra varandra. Om huset rivs, byggs ett nytt - men det Ã¤r inte samma hus.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      POD        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Container â”‚  â”‚  <- Vanligast: en container
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  IP: 10.0.1.5   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      POD        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   App     â”‚  â”‚  <- Ibland: flera containers
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Sidecar  â”‚  â”‚  <- Ex: logging agent
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  IP: 10.0.1.6   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exempel pod:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: myapp
      image: myapp:1.0
      ports:
        - containerPort: 8080
```

Men du kommer sÃ¤llan att skapa pods direkt. IstÃ¤llet anvÃ¤nder du Deployments...

### Node - Arbetsmaskinen

En **Node** Ã¤r en fysisk eller virtuell maskin i ditt cluster. Det Ã¤r dÃ¤r pods faktiskt kÃ¶rs.

**Typer:**

- Master nodes (kÃ¶r control plane)
- Worker nodes (kÃ¶r dina pods)

**Du hanterar vanligtvis inte nodes direkt** - Kubernetes skÃ¶ter placeringen. Du sÃ¤ger "kÃ¶r 5 pods" och K8s bestÃ¤mmer vilka nodes de ska kÃ¶ra pÃ¥.

### Cluster - Hela MiljÃ¶n

Ett **Cluster** Ã¤r hela din Kubernetes-miljÃ¶:

- Control plane
- Alla worker nodes
- Allt som kÃ¶rs pÃ¥ dem

Ett cluster kan ha 3 nodes eller 1000 nodes - det Ã¤r samma koncept.

## Deployment - SÃ¥ Hanterar Du Pods

Okej, sÃ¥ pods Ã¤r viktiga - men hur skapar och hanterar man dem? Svaret Ã¤r **Deployment**.

### Vad Ã¤r en Deployment?

En Deployment Ã¤r ett **deklarativt sÃ¤tt** att hantera pods. Du beskriver vad du vill ha, och Kubernetes gÃ¶r det verkligt.

**Exempel:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 3 # Jag vill ha 3 pods
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: myapp:1.0
          ports:
            - containerPort: 8080
```

Med detta sÃ¤ger du:

- "Jag vill ha 3 identiska pods"
- "De ska kÃ¶ra imagen myapp:1.0"
- "De ska lyssna pÃ¥ port 8080"

Kubernetes tar det hÃ¤rifrÃ¥n och:

1. Skapar 3 pods
2. FÃ¶rdelar dem Ã¶ver worker nodes
3. Ã–vervakar dem kontinuerligt
4. Om en pod dÃ¶r, skapar K8s en ny automatiskt

**Detta Ã¤r self-healing!**

### Self-Healing i Aktion

```
Scenario:
Ã–nskat tillstÃ¥nd: 3 pods
Faktiskt tillstÃ¥nd: 3 pods âœ“

Node 2 kraschar!
Faktiskt tillstÃ¥nd: 1 pod (2 pods dog) âœ—

Kubernetes upptÃ¤cker:
"HallÃ¥, jag ska ha 3 pods men har bara 1!"

Kubernetes agerar:
- Skapar 2 nya pods pÃ¥ andra nodes
- Faktiskt tillstÃ¥nd: 3 pods âœ“

Allt fixat - automatiskt!
```

### ReplicaSet - Bakom Kulisserna

NÃ¤r du skapar en Deployment skapar Kubernetes automatiskt en **ReplicaSet**. ReplicaSet Ã¤r den som faktiskt sÃ¤kerstÃ¤ller att rÃ¤tt antal pods kÃ¶rs.

```
Deployment â†’ ReplicaSet â†’ Pods (3 st)
```

Du behÃ¶ver sÃ¤llan interagera med ReplicaSets direkt - Deployment hanterar det Ã¥t dig.

**VarfÃ¶r denna hierarki?**

- Deployment = hanterar updates och rollbacks
- ReplicaSet = hanterar antalet replicas
- Pods = kÃ¶r faktiska containers

## Service - Stabil NÃ¤tverksÃ¥tkomst

Nu har vi 3 pods som kÃ¶r vÃ¥r app. Men det finns ett problem: **pods Ã¤r ephemeral**.

### Problemet

- Pod 1 har IP 10.0.1.5
- Pod 1 kraschar
- Ny pod skapas med IP 10.0.1.9
- Alla som pratar med 10.0.1.5 kommer nu inte hitta appen!

Dessutom, vi har 3 pods - vilken ska man prata med?

### LÃ¶sningen - Service

En **Service** ger en stabil IP-adress och DNS-namn fÃ¶r att komma Ã¥t dina pods. Den fungerar ocksÃ¥ som en inbyggd load balancer.

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   SERVICE   â”‚
          â”‚ myapp-svc   â”‚
          â”‚ 10.100.0.5  â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”     â”Œâ”€â”€â”€â–¼â”€â”€â”     â”Œâ”€â”€â”€â–¼â”€â”€â”
â”‚ Pod1 â”‚     â”‚ Pod2 â”‚     â”‚ Pod3 â”‚
â”‚10.0.1â”‚     â”‚10.0.2â”‚     â”‚10.0.3â”‚
â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
```

**Med en Service:**

- Alla pratar med servicens IP (10.100.0.5) eller DNS namn (myapp-svc)
- Service load balancerar automatiskt till alla pods
- Pods kan dÃ¶ och skapas - service IP Ã¤ndras aldrig
- Built-in service discovery!

### Service Types

**ClusterIP (default)**

- Ã…tkomst endast inom clustret
- Perfekt fÃ¶r interna services

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: ClusterIP
  selector:
    app: myapp # Hittar alla pods med label app=myapp
  ports:
    - port: 80
      targetPort: 8080 # Pods lyssnar pÃ¥ 8080
```

**NodePort**

- Exponerar service pÃ¥ varje nodes IP pÃ¥ en specifik port
- Kan nÃ¥s utifrÃ¥n clustret
- Port range: 30000-32767

```yaml
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30080 # Ã…tkomst via <any-node-ip>:30080
```

**LoadBalancer**

- Skapar en cloud load balancer (AWS ELB, GCP LB, etc.)
- Extern IP-adress
- AnvÃ¤nds fÃ¶r att exponera apps till internet

```yaml
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080
```

**ExternalName**

- DNS alias till extern service
- AnvÃ¤nds sÃ¤llan

### Hur Service Hittar Pods

Services anvÃ¤nder **labels** fÃ¶r att hitta pods:

```yaml
# Deployment skapar pods med label app=myapp
template:
  metadata:
    labels:
      app: myapp

# Service hittar dessa pods via selector
selector:
  app: myapp
```

Service skickar trafik till alla pods som matchar labeln. NÃ¤r nya pods skapas eller gamla tas bort, uppdateras Service automatiskt!

## Namespace - Virtuella Cluster

Ett **Namespace** Ã¤r ett sÃ¤tt att dela upp ditt cluster i virtuella sub-clusters. Det Ã¤r som rum i ett stort hus.

### VarfÃ¶r Namespaces?

- **Organisera resurser**: Dev, staging, production i samma cluster
- **Isolera team**: Team A och Team B fÃ¥r egna namespaces
- **Resource quotas**: BegrÃ¤nsa hur mycket CPU/minne varje namespace kan anvÃ¤nda
- **Access control**: Olika anvÃ¤ndare fÃ¥r access till olika namespaces

```
KUBERNETES CLUSTER
â”œâ”€â”€ namespace: development
â”‚   â”œâ”€â”€ myapp-deployment
â”‚   â”œâ”€â”€ myapp-service
â”‚   â””â”€â”€ database
â”œâ”€â”€ namespace: staging
â”‚   â”œâ”€â”€ myapp-deployment
â”‚   â”œâ”€â”€ myapp-service
â”‚   â””â”€â”€ database
â””â”€â”€ namespace: production
    â”œâ”€â”€ myapp-deployment
    â”œâ”€â”€ myapp-service
    â””â”€â”€ database
```

**Default namespaces:**

- `default` - om du inte anger namespace
- `kube-system` - Kubernetes system components
- `kube-public` - publikt tillgÃ¤nglig data
- `kube-node-lease` - node heartbeats

**Skapa namespace:**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: development
```

**AnvÃ¤nda namespace:**

```bash
kubectl get pods -n development
kubectl apply -f deployment.yaml -n production
```

## ConfigMap och Secret

Din applikation behÃ¶ver konfiguration. Men du vill inte baka in den i Docker imagen - dÃ¥ mÃ¥ste du bygga om imagen fÃ¶r varje miljÃ¶!

### ConfigMap - Icke-KÃ¤nslig Konfiguration

**ConfigMap** lagrar konfigurationsdata som kan injiceras i pods.

**Exempel:**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  DATABASE_URL: "postgres://db:5432/mydb"
  API_ENDPOINT: "https://api.example.com"
  FEATURE_FLAG_X: "true"
  log_level: "info"
```

**AnvÃ¤nda i pod:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
    - name: myapp
      image: myapp:1.0
      envFrom:
        - configMapRef:
            name: app-config # Alla keys blir env variables
```

Nu har containern miljÃ¶variabler:

- `DATABASE_URL=postgres://db:5432/mydb`
- `API_ENDPOINT=https://api.example.com`
- osv.

**FÃ¶rdelar:**

- Samma imagen i dev, staging, production
- Ã„ndra config utan att rebuilda
- Versionshantera config separat

### Secret - KÃ¤nslig Data

**Secret** Ã¤r som ConfigMap men fÃ¶r kÃ¤nslig data: lÃ¶senord, API-nycklar, certifikat.

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  username: YWRtaW4= # base64 encoded
  password: c3VwZXJzZWNyZXQ= # base64 encoded
```

**AnvÃ¤nda i pod:**

```yaml
spec:
  containers:
    - name: myapp
      image: myapp:1.0
      env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
```

**OBS:** Secrets Ã¤r base64-encodade, **inte krypterade** by default. FÃ¶r riktig sÃ¤kerhet behÃ¶ver du:

- Encryption at rest (konfigurera etcd encryption)
- RBAC fÃ¶r att begrÃ¤nsa access
- Eller externa secret managers (HashiCorp Vault, AWS Secrets Manager)

## Persistent Volumes - Lagring som Ã–verlever

Containers Ã¤r ephemeral - nÃ¤r de dÃ¶r, fÃ¶rsvinner all data. Men vad hÃ¤nder med din databas?

### Problemet

```
Pod med PostgreSQL:
- Sparar data i /var/lib/postgresql/data
- Pod kraschar
- Ny pod startas
- All data Ã¤r borta! ğŸ˜±
```

### LÃ¶sningen - Persistent Volumes

**PersistentVolume (PV)**

- Ett lagringsutrymme i clustret
- Kan vara: cloud disk (AWS EBS, GCP Persistent Disk), NFS, local disk, etc.
- Ã–verlever pods

**PersistentVolumeClaim (PVC)**

- En "fÃ¶rfrÃ¥gan" om lagring
- "Jag behÃ¶ver 10GB disk"
- Kubernetes binder PVC till en PV

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       POD        â”‚
â”‚   PostgreSQL     â”‚
â”‚       â”‚          â”‚
â”‚       â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   PVC    â”‚    â”‚  <- Pod anvÃ¤nder PVC
â”‚  â”‚  10GB    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   PV    â”‚         <- PVC bunden till PV
   â”‚  20GB   â”‚
   â”‚ AWS EBS â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exempel PVC:**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce # Endast en node kan skriva
  resources:
    requests:
      storage: 10Gi # BehÃ¶ver 10GB
```

**AnvÃ¤nda i Deployment:**

```yaml
spec:
  containers:
    - name: postgres
      image: postgres:15
      volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumes:
    - name: postgres-storage
      persistentVolumeClaim:
        claimName: postgres-pvc
```

Nu Ã¶verlever databasens data Ã¤ven om poden dÃ¶r!

## Labels och Selectors

**Labels** Ã¤r key-value pairs som du kan sÃ¤tta pÃ¥ vilka Kubernetes-objekt som helst. De Ã¤r fundamentala fÃ¶r hur K8s organiserar och hittar saker.

### Labels

```yaml
metadata:
  name: myapp-pod
  labels:
    app: myapp
    environment: production
    version: "1.0"
    team: backend
```

**VarfÃ¶r labels?**

- Organisera resurser
- Gruppera relaterade objekt
- MÃ¶jliggÃ¶r selectors

### Selectors

**Selectors** anvÃ¤nds fÃ¶r att hitta objekt baserat pÃ¥ labels:

```yaml
# Service hittar alla pods med app=myapp
selector:
  app: myapp

# Deployment skapar pods med dessa labels
template:
  metadata:
    labels:
      app: myapp
```

**Exempel:**

```
Pods:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod: frontend-1  â”‚
â”‚ Labels:          â”‚
â”‚  app=frontend    â”‚
â”‚  tier=frontend   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod: frontend-2  â”‚
â”‚ Labels:          â”‚
â”‚  app=frontend    â”‚
â”‚  tier=frontend   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod: backend-1   â”‚
â”‚ Labels:          â”‚
â”‚  app=backend     â”‚
â”‚  tier=backend    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Service med selector app=frontend â†’ hittar frontend-1 och frontend-2
Service med selector app=backend  â†’ hittar backend-1
```

Labels Ã¤r kraftfulla! Du kan vÃ¤lja vilka pods som helst:

- `app=myapp` - alla myapp pods
- `environment=production` - alla production pods
- `app=myapp,environment=production` - myapp pods i production

## Hur Kubernetes UpprÃ¤tthÃ¥ller Ã–nskat TillstÃ¥nd

Kubernetes jobbar enligt **deklarativt paradigm**:

- Du deklarerar vad du vill ha
- Kubernetes figur out hur det ska gÃ¶ras
- Kubernetes Ã¶vervakar kontinuerligt och korrigerar avvikelser

### Deklarativt vs Imperativt

**Imperativt (hur):**

```bash
docker run myapp:1.0
docker run myapp:1.0
docker run myapp:1.0
# Du sÃ¤ger exakt VAD som ska gÃ¶ras, steg fÃ¶r steg
```

**Deklarativt (vad):**

```yaml
replicas: 3
image: myapp:1.0
# Du sÃ¤ger VAD du vill ha, K8s figurer ut hur
```

### Control Loop

Kubernetes kÃ¶r en kontinuerlig loop:

```
1. LÃ¤s Ã¶nskat tillstÃ¥nd (desired state)
   "replicas: 3"

2. Observera faktiskt tillstÃ¥nd (actual state)
   "2 pods kÃ¶rs"

3. JÃ¤mfÃ¶r
   Desired: 3
   Actual: 2
   Diff: -1

4. Agera
   Skapa 1 ny pod

5. Repetera (konstant)
```

### Exempel Scenario: Self-Healing

```
Klockan 09:00:
âœ“ Ã–nskat: 3 pods
âœ“ Faktiskt: 3 pods
âœ“ Status: OK

Klockan 14:32:
âœ“ Ã–nskat: 3 pods
âœ— Faktiskt: 2 pods (en krashade)
âš  Controller upptÃ¤cker avvikelse
â†’ Skapar ny pod

Klockan 14:32:30:
âœ“ Ã–nskat: 3 pods
âœ“ Faktiskt: 3 pods
âœ“ Status: OK igen
```

**Detta hÃ¤nder automatiskt, 24/7. Detta Ã¤r self-healing!**

Du sover, men Kubernetes vaktar. En server gÃ¥r ner kl 03:00? Kubernetes flyttar pods till andra servrar. En pod kraschar? Kubernetes startar en ny. Allt utan att du lyfter ett finger.

## Scaling i Kubernetes

Skalning Ã¤r en av Kubernetes stÃ¶rsta styrkor.

### Manual Scaling

**Ã„ndra antal replicas:**

```bash
kubectl scale deployment myapp --replicas=5
```

Eller uppdatera YAML:

```yaml
spec:
  replicas: 5 # Var 3, nu 5
```

Kubernetes ser fÃ¶rÃ¤ndringen:

- "Jag har 3 pods men ska ha 5"
- Skapar 2 nya pods
- FÃ¶rdelar dem Ã¶ver nodes
- Klart!

### Auto-Scaling (HPA - Horizontal Pod Autoscaler)

VarfÃ¶r ska du manuellt skala nÃ¤r Kubernetes kan gÃ¶ra det automatiskt?

**HPA** Ã¶vervakar metrics och skalar automatiskt:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deployment
  minReplicas: 2 # Minst 2 pods
  maxReplicas: 10 # Max 10 pods
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70 # Skala om CPU > 70%
```

**Hur det fungerar:**

```
Normal trafik:
CPU usage: 40%
Pods: 2 (minReplicas)

Trafikspike! Black Friday!
CPU usage: 85%
HPA: "Ã–ver 70%! Skala upp!"
Pods: 2 â†’ 4 â†’ 6

CPU usage sjunker till 60%
Pods: 6 (behÃ¥lls tills det sjunker mer)

Trafik normaliseras
CPU usage: 30%
HPA: "Under 70%, skala ner"
Pods: 6 â†’ 4 â†’ 2

Tillbaka till normal!
```

**FÃ¶rdelar:**

- Hanterar trafikspikes automatiskt
- Sparar pengar (fÃ¤rre pods nÃ¤r det Ã¤r lugnt)
- Du behÃ¶ver inte Ã¶vervaka konstant

### Cluster Auto-Scaling

Om alla nodes Ã¤r fulla, vad dÃ¥? **Cluster Autoscaler** kan lÃ¤gga till fler nodes automatiskt (i cloud).

```
Scenario:
- 3 nodes, alla fulla
- HPA vill starta fler pods
- Men det finns inget utrymme!

Cluster Autoscaler:
- UpptÃ¤cker att pods Ã¤r "pending" (vÃ¤ntar pÃ¥ plats)
- BegÃ¤r ny node frÃ¥n cloud provider
- AWS/GCP startar ny VM
- Ny node joins cluster
- Pods schemalÃ¤ggs pÃ¥ nya noden
- Problem lÃ¶st!
```

Detta Ã¤r **true auto-scaling** - bÃ¥de pods OCH infrastruktur skalar automatiskt.

## Rolling Updates - Zero Downtime Deployments

En av Kubernetes coolaste features: uppdatera din app utan downtime.

### Problemet med Traditionella Updates

```
Traditionellt:
1. Stoppa gammal version
2. Deploy ny version
3. Starta ny version
   
   â¸ DOWNTIME under step 2-3!
```

### Kubernetes Rolling Update

```yaml
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1 # Max 1 extra pod under update
      maxUnavailable: 1 # Max 1 pod kan vara nere
```

**Process:**

```
Initial: v1.0
[v1] [v1] [v1] [v1]

Step 1: Starta en v2 pod
[v1] [v1] [v1] [v1] [v2]  <- maxSurge: 1

Step 2: VÃ¤nta tills v2 Ã¤r ready
[v1] [v1] [v1] [v1] [v2âœ“]

Step 3: Ta bort en v1 pod
[v1] [v1] [v1] [v2âœ“]

Step 4: Starta ytterligare en v2
[v1] [v1] [v1] [v2âœ“] [v2]

Step 5: v2 ready, ta bort v1
[v1] [v1] [v2âœ“] [v2âœ“]

Step 6-8: FortsÃ¤tt...
[v1] [v2âœ“] [v2âœ“] [v2]
[v2âœ“] [v2âœ“] [v2âœ“] [v2]

Slutresultat:
[v2âœ“] [v2âœ“] [v2âœ“] [v2âœ“]
```

**Under hela processen:**

- Minst 3 pods Ã¤r alltid running (maxUnavailable: 1)
- Users upplever ingen downtime
- Service load balancerar mellan v1 och v2 pods

### Rollback

Vad hÃ¤nder om v2 har en bugg?

```bash
kubectl rollout undo deployment myapp-deployment
```

Kubernetes rullar tillbaka till fÃ¶regÃ¥ende version - samma rolling update process, men baklÃ¤nges!

```
[v2] [v2] [v2] [v2]
â†’ Rolling back...
[v2] [v2] [v2] [v1]
[v2] [v2] [v1] [v1]
[v2] [v1] [v1] [v1]
[v1] [v1] [v1] [v1]
Rollback komplett!
```

## Health Checks - SÃ¥ Kubernetes Vet Om Du Ã„r OK

Hur vet Kubernetes om en container Ã¤r healthy? Health checks!

### Liveness Probe - Ã„r du vid liv?

**Liveness probe** kollar om containern Ã¤r vid liv. Om den failar, startar Kubernetes om containern.

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30 # VÃ¤nta 30s efter start
  periodSeconds: 10 # Kolla var 10:e sekund
  timeoutSeconds: 5 # Timeout efter 5s
  failureThreshold: 3 # 3 misslyckanden â†’ restart
```

**Exempel scenario:**

```
App startar â†’ OK
Health check: GET /health â†’ 200 OK âœ“
Health check: GET /health â†’ 200 OK âœ“
Health check: GET /health â†’ 200 OK âœ“

App fÃ¥r deadlock, hÃ¤nger sig
Health check: GET /health â†’ Timeout âœ— (1)
Health check: GET /health â†’ Timeout âœ— (2)
Health check: GET /health â†’ Timeout âœ— (3)

failureThreshold nÃ¥tt!
Kubernetes: "Container Ã¤r dÃ¶d, restartar!"
Container restartas
App startar â†’ OK igen
```

### Readiness Probe - Ã„r du redo fÃ¶r trafik?

**Readiness probe** kollar om containern Ã¤r redo att ta emot requests. Om den failar, tar Kubernetes bort poden frÃ¥n Service (ingen trafik skickas).

```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

**VarfÃ¶r Ã¤r detta annorlunda Ã¤n liveness?**

```
Pod startar:
- Liveness: OK (process kÃ¶rs)
- Readiness: FAIL (databas connection inte etablerad Ã¤n)
â†’ Pod Ã¤r alive men fÃ¥r INGEN trafik

5 sekunder senare:
- Liveness: OK
- Readiness: OK (databas connection OK nu)
â†’ Pod fÃ¥r nu trafik frÃ¥n Service
```

**AnvÃ¤ndningsfall:**

- VÃ¤nta pÃ¥ databas connection
- Ladda cache vid uppstart
- VÃ¤nta pÃ¥ dependencies

### Startup Probe - FÃ¶r LÃ¥ngsamma Uppstarter

Vissa apps tar lÃ¥ng tid att starta (JVM anyone?). **Startup probe** ger mer tid fÃ¶r uppstart.

```yaml
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  failureThreshold: 30 # 30 fÃ¶rsÃ¶k
  periodSeconds: 10 # Var 10:e sekund
  # = Max 300 sekunder (5 min) fÃ¶r uppstart
```

Efter startup probe lyckas, tar liveness probe Ã¶ver.

## Load Balancing - Inbyggt i Kubernetes

Med Docker Compose mÃ¥ste du sÃ¤tta upp nginx eller liknande fÃ¶r load balancing. I Kubernetes? Inbyggt!

### Service = Load Balancer

NÃ¤r du skapar en Service fÃ¥r du automatiskt load balancing:

```
Request â†’ Service
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“         â†“
  Pod 1     Pod 2     Pod 3
```

**Load balancing algorithms:**

- Round-robin (default) - turnerar mellan pods
- Session affinity - samma client â†’ samma pod (optional)

**Exempel:**

```
Request 1 â†’ Pod 1
Request 2 â†’ Pod 2
Request 3 â†’ Pod 3
Request 4 â†’ Pod 1
Request 5 â†’ Pod 2
...
```

### Kube-proxy

**Kube-proxy** kÃ¶rs pÃ¥ varje node och implementerar load balancing:

- Lyssnar pÃ¥ Service-Ã¤ndringar
- Uppdaterar iptables/ipvs rules
- Dirigerar trafik till rÃ¤tt pods

Du behÃ¶ver inte tÃ¤nka pÃ¥ det - det funkar bara!

## Ingress - HTTP Routing

Service ger dig load balancing, men vad om du vill:

- Routea baserat pÃ¥ URL path?
- SSL/TLS termination?
- En entry point fÃ¶r mÃ¥nga services?

Det Ã¤r hÃ¤r **Ingress** kommer in!

### Vad Ã¤r Ingress?

Ingress Ã¤r som en API Gateway eller reverse proxy fÃ¶r ditt cluster.

```
               INTERNET
                  â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   INGRESS    â”‚  <- En extern IP
           â”‚  Controller  â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“            â†“            â†“
[Service]    [Service]    [Service]
frontend     user-api     product-api
```

### Path-Based Routing

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
spec:
  rules:
    - host: myapp.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: frontend-service
                port:
                  number: 80
          - path: /api/users
            pathType: Prefix
            backend:
              service:
                name: user-service
                port:
                  number: 8080
          - path: /api/products
            pathType: Prefix
            backend:
              service:
                name: product-service
                port:
                  number: 8080
```

**Routing:**

```
myapp.com/              â†’ frontend-service
myapp.com/api/users     â†’ user-service
myapp.com/api/products  â†’ product-service
```

En IP-adress, flera services - snyggt!

### SSL/TLS Termination

```yaml
spec:
  tls:
  - hosts:
    - myapp.com
    secretName: tls-secret    # Certificate i Secret
  rules:
  - host: myapp.com
    ...
```

HTTPS termineras vid Ingress, intern trafik kan vara HTTP.

### Ingress Controller

FÃ¶r att Ingress ska fungera behÃ¶ver du en **Ingress Controller**:

- nginx-ingress (populÃ¤rast)
- Traefik
- HAProxy
- Cloud-specific (AWS ALB, GCP LB)

Ingress Controller Ã¤r den faktiska load balancern som implementerar Ingress rules.

## Resource Management - CPU och Minne

Hur fÃ¶rhindrar du att en app tar alla resurser?

### Requests och Limits

**Requests** = minimum som garanteras
**Limits** = maximum som tillÃ¥ts

```yaml
resources:
  requests:
    cpu: 100m # 100 millicores = 0.1 CPU
    memory: 128Mi # 128 megabytes
  limits:
    cpu: 500m # Max 0.5 CPU
    memory: 512Mi # Max 512 megabytes
```

### Hur Kubernetes AnvÃ¤nder Detta

**Scheduler:**

```
Pod behÃ¶ver: requests.cpu = 100m, requests.memory = 128Mi

Node 1: 
  Kapacitet: 2 CPU, 4Gi memory
  AnvÃ¤nt: 1.9 CPU, 3.5Gi memory
  Ledigt: 0.1 CPU, 0.5Gi memory
  â†’ NEJ, inte nog med CPU

Node 2:
  Kapacitet: 4 CPU, 8Gi memory
  AnvÃ¤nt: 1 CPU, 2Gi memory
  Ledigt: 3 CPU, 6Gi memory
  â†’ JA! SchemalÃ¤gg hÃ¤r
```

**Runtime:**

- Container anvÃ¤nder upp till `limits.cpu` och `limits.memory`
- Om den fÃ¶rsÃ¶ker anvÃ¤nda mer minne â†’ OOMKilled (killed)
- Om den fÃ¶rsÃ¶ker anvÃ¤nda mer CPU â†’ throttled (bromsas)

### Best Practices

```yaml
# Development pod - inga hÃ¥rda limits
resources:
  requests:
    cpu: 100m
    memory: 128Mi

# Production pod - requests = limits fÃ¶r fÃ¶rutsÃ¤gbarhet
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 512Mi
```

## Kubernetes i Praktiken - Workflow

Hur jobbar man egentligen med Kubernetes dagligen?

### Development Workflow

```
1. Utveckla kod lokalt
   â†“
2. Bygg Docker image
   docker build -t myapp:1.1 .
   â†“
3. Pusha till registry
   docker push myregistry.com/myapp:1.1
   â†“
4. Uppdatera Kubernetes Deployment
   # Ã„ndra image: myapp:1.0 â†’ myapp:1.1 i YAML
   â†“
5. Applya Ã¤ndring
   kubectl apply -f deployment.yaml
   â†“
6. Kubernetes gÃ¶r rolling update
   - Startar pods med nya imagen
   - Tar bort gamla pods gradvis
   â†“
7. Verifiera
   kubectl get pods
   kubectl logs <pod-name>
```

### Infrastructure as Code

Alla Kubernetes-resurser definieras i YAML-filer:

```
my-app/
â”œâ”€â”€ deployment.yaml
â”œâ”€â”€ service.yaml
â”œâ”€â”€ ingress.yaml
â”œâ”€â”€ configmap.yaml
â””â”€â”€ secret.yaml
```

**FÃ¶rdelar:**

- Versionshantera i Git
- Code review av infrastruktur
- Reproducerbar miljÃ¶
- Disaster recovery - replaya alla YAML-filer

**Deploy allt:**

```bash
kubectl apply -f my-app/
```

### GitOps Workflow

Modern approach:

1. Commit YAML till Git
2. CI/CD pipeline detekterar Ã¤ndring
3. Automatisk deploy till K8s
4. Git = single source of truth

```
Git commit â†’ CI/CD â†’ kubectl apply â†’ Kubernetes
```

## kubectl - Kommandoradsverktyget

**kubectl** Ã¤r ditt verktyg fÃ¶r att interagera med Kubernetes.

### Vanliga Kommandon

**Lista resurser:**

```bash
kubectl get pods
kubectl get deployments
kubectl get services
kubectl get nodes
kubectl get all    # Allt i current namespace
```

**Detaljerad info:**

```bash
kubectl describe pod myapp-pod-abc123
kubectl describe deployment myapp-deployment
```

**Logs:**

```bash
kubectl logs myapp-pod-abc123
kubectl logs -f myapp-pod-abc123    # Follow (live)
kubectl logs myapp-pod-abc123 -c container-name    # Specific container
```

**Exec (shell into pod):**

```bash
kubectl exec -it myapp-pod-abc123 -- /bin/bash
kubectl exec -it myapp-pod-abc123 -- sh
```

**Apply/Create/Delete:**

```bash
kubectl apply -f deployment.yaml      # Create eller update
kubectl create -f deployment.yaml     # Endast create
kubectl delete -f deployment.yaml     # Delete
```

**Scaling:**

```bash
kubectl scale deployment myapp --replicas=5
```

**Namespace:**

```bash
kubectl get pods -n production              # Specific namespace
kubectl get pods --all-namespaces          # Alla namespaces
kubectl config set-context --current --namespace=production  # SÃ¤tt default
```

**Debugging:**

```bash
kubectl get events                          # Cluster events
kubectl top pods                            # Resource usage
kubectl port-forward myapp-pod-abc123 8080:8080    # Local access
```

### Kubectl Tips

```bash
# Alias fÃ¶r snabbhet
alias k=kubectl
k get pods

# Watch mode
kubectl get pods --watch

# Output formats
kubectl get pods -o wide              # Mer info
kubectl get pods -o yaml              # YAML format
kubectl get pods -o json              # JSON format

# Imperative commands (snabbt test)
kubectl run nginx --image=nginx       # Snabb pod
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port=80
```

## Managed Kubernetes Services

Att kÃ¶ra eget Kubernetes cluster Ã¤r komplext - control plane, etcd, networking, upgraderingar...

DÃ¤rfÃ¶r anvÃ¤nder de flesta **managed Kubernetes** frÃ¥n cloud providers.

### Google Kubernetes Engine (GKE)

**FÃ¶rdelar:**

- Skapades av Google (som uppfann K8s)
- Enklast att komma igÃ¥ng
- Automatiska upgrades
- God integration med Google Cloud

```bash
gcloud container clusters create my-cluster --num-nodes=3
```

### Amazon Elastic Kubernetes Service (EKS)

**FÃ¶rdelar:**

- AWS's K8s-tjÃ¤nst
- Integration med AWS services (RDS, S3, IAM, etc.)
- Stor community

```bash
eksctl create cluster --name my-cluster --nodes=3
```

### Azure Kubernetes Service (AKS)

**FÃ¶rdelar:**

- Microsoft Azure's K8s
- Bra fÃ¶r fÃ¶retag med Microsoft-stack
- Integration med Azure services

```bash
az aks create --resource-group myRG --name myCluster --node-count 3
```

### FÃ¶rdelar med Managed K8s

- **Control plane hanteras fÃ¶r dig** - du slipper skÃ¶ta master nodes
- **Automatiska updates** - K8s uppdateras sÃ¤kert
- **Enklare setup** - klart pÃ¥ minuter istÃ¤llet fÃ¶r dagar
- **BÃ¤ttre sÃ¤kerhet** - patching, RBAC, network policies
- **Du betalar bara fÃ¶r worker nodes** - control plane ofta gratis

**Men:**

- Vendor lock-in (lite)
- Kostar mer Ã¤n self-hosted (men du sparar tid)
- Mindre kontroll Ã¶ver control plane

## Local Kubernetes fÃ¶r Utveckling

Du vill testa K8s lokalt innan du kÃ¶r i produktion? Det finns flera alternativ.

### Minikube

**Minikube** = Single-node K8s cluster pÃ¥ din laptop.

```bash
# Installera
brew install minikube    # macOS
choco install minikube   # Windows

# Starta
minikube start

# AnvÃ¤nd
kubectl get nodes
```

**FÃ¶rdelar:**

- LÃ¤tt att komma igÃ¥ng
- Perfekt fÃ¶r learning
- Addons fÃ¶r vanliga verktyg

### Kind (Kubernetes in Docker)

**Kind** = KÃ¶r K8s cluster i Docker containers.

```bash
# Installera
brew install kind

# Skapa cluster
kind create cluster

# Multi-node cluster
kind create cluster --config multi-node.yaml
```

**FÃ¶rdelar:**

- Supersnabbt
- Lightweight
- Multi-node support
- Perfekt fÃ¶r CI/CD testing

### Docker Desktop

**Docker Desktop** inkluderar single-node Kubernetes.

- Enable Kubernetes i settings
- Klart!

**FÃ¶rdelar:**

- Enklast - redan installerat med Docker Desktop
- Bra fÃ¶r basic testing

### K3s

**K3s** = Lightweight Kubernetes fÃ¶r edge, IoT, utveckling.

```bash
curl -sfL https://get.k3s.io | sh -
```

**FÃ¶rdelar:**

- Mindre Ã¤n 100MB binary
- KÃ¶r pÃ¥ Raspberry Pi
- Snabb startup

## NÃ¤r BehÃ¶ver Du FAKTISKT Kubernetes?

Kubernetes Ã¤r coolt, men Ã¤r det rÃ¤tt fÃ¶r dig?

### Du BEHÃ–VER Troligtvis K8s Om:

âœ… **Du kÃ¶r microservices i stor skala**

- 10+ services
- Varje service behÃ¶ver skalas oberoende

âœ… **Flera team deployer oberoende**

- Team A deployer sin service utan att vÃ¤nta pÃ¥ Team B

âœ… **High availability Ã¤r kritiskt**

- 99.9%+ uptime requirement
- Kan inte ha downtime

âœ… **Multi-environment pÃ¥ mÃ¥nga servrar**

- Dev, staging, production
- Varje miljÃ¶ har 10+ servrar

âœ… **Auto-scaling Ã¤r requirement**

- Trafiken varierar mycket
- Black Friday, rush hours, etc.

âœ… **Du har DevOps-expertis**

- Team som kan hantera K8s komplexitet

### Du behÃ¶ver INTE K8s Om:

âŒ **Liten applikation eller team**

- Monolith eller 1-3 services
- 1-5 utvecklare

âŒ **Docker Compose rÃ¤cker**

- KÃ¶r pÃ¥ 1-2 servrar
- Simpel setup

âŒ **Saknar K8s-expertis**

- Ingen har kÃ¶rt K8s innan
- Ingen tid att lÃ¤ra sig

âŒ **Kan inte motivera complexity**

- Operational overhead
- Monitoring, logging setup
- Network complexity
- Learning curve

### Kostnad-Nytta Analys

**Kubernetes kostar:**

- Learning time (veckor/mÃ¥nader)
- Operational complexity
- Monitoring/logging infrastructure
- CI/CD pipelines
- Network complexity
- FelsÃ¶kning svÃ¥rare

**Kubernetes ger:**

- Auto-scaling
- Self-healing
- Zero-downtime deployments
- Service discovery
- Multi-cloud flexibility
- Industry standard

**FrÃ¥ga dig sjÃ¤lv:** Ã„r fÃ¶rdelarna vÃ¤rda kostnaden?

### Tomrumregel

```
1-3 servers + monolith/few services
  â†’ Docker Compose
  
4-10 servers + microservices
  â†’ Kanske K8s, men tÃ¤nk noga
  
10+ servers + mÃ¥nga microservices
  â†’ Definitivt K8s
```

**Viktig poÃ¤ng:** AnvÃ¤nd inte Kubernetes bara fÃ¶r att det Ã¤r coolt eller "industry standard". AnvÃ¤nd det nÃ¤r du faktiskt behÃ¶ver det!

## Complexity Cost - Var Medveten

Kubernetes Ã¤r powerful, men det kommer med pris.

### Complexity Factors

**Learning Curve**

- Nya koncept: Pods, Services, Deployments, etc.
- kubectl commands
- YAML configurations
- Networking concepts
- = Veckor/mÃ¥nader fÃ¶r teamet

**Operational Overhead**

- Cluster management
- Upgrades och patching
- Monitoring setup (Prometheus, Grafana)
- Logging setup (ELK/EFK stack)
- Backup och disaster recovery

**Network Complexity**

- CNI plugins (Calico, Flannel, etc.)
- Service mesh (Istio, Linkerd) om du behÃ¶ver det
- Ingress controllers
- Network policies
- = Networking-expertis krÃ¤vs

**Security**

- RBAC konfiguration
- Pod security policies
- Network policies
- Secret management
- Image scanning
- = Security-expertis krÃ¤vs

**Debugging**

- SvÃ¥rare Ã¤n Docker Compose
- Fler lager att debugga
- Pods kan vara pÃ¥ olika nodes
- Logs Ã¶ver flera pods

### Ã„r Det VÃ¤rt Det?

**Om du Ã¤r ett stort fÃ¶retag med microservices:**

- Ja! FÃ¶rdelarna Ã¶vervÃ¤ger complexity

**Om du Ã¤r en startup med 3 utvecklare:**

- Troligen nej. Fokusera pÃ¥ produkt, anvÃ¤nd enklare lÃ¶sningar

**Om du Ã¤r mellan:**

- Ã–vervÃ¤g managed K8s (GKE, EKS, AKS)
- De abstractar bort mycket complexity
- Du fÃ¥r fÃ¶rdelarna utan full operational overhead

## Kubernetes och Microservices

Kubernetes och microservices Ã¤r som gjorda fÃ¶r varandra.

### VarfÃ¶r Passar De Perfekt?

**Microservices behÃ¶ver:**

- Oberoende deployment âœ“ K8s Deployments
- Individuell skalning âœ“ K8s auto-scaling
- Service discovery âœ“ K8s Services
- Load balancing âœ“ K8s Services
- Health monitoring âœ“ K8s probes
- Resilience âœ“ K8s self-healing

**Kubernetes ger:**

- Allt ovan!

### Microservice Architecture pÃ¥ K8s

```
                     INGRESS
                        â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“                   â†“
        API GATEWAY          FRONTEND
              â†“                   
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“         â†“
USER-SVC  PRODUCT-SVC  ORDER-SVC
    â†“         â†“         â†“
  REDIS   POSTGRES   POSTGRES
```

**Varje service:**

- Egen Deployment (skalas oberoende)
- Egen Service (intern communication)
- Egna ConfigMaps/Secrets
- Egen databas (database per service pattern)

**Detta Ã¤r dÃ¤r Kubernetes shinar!**

## Monitoring och Logging i Kubernetes

I produktion mÃ¥ste du Ã¶vervaka ditt cluster.

### Monitoring - Prometheus + Grafana

**Prometheus** = Metrics collection
**Grafana** = Visualization

```
Kubernetes Cluster
    â†“ (metrics)
Prometheus
    â†“ (query)
Grafana Dashboards
    â†“ (view)
DevOps Team
```

**Vad monitora:**

- **Cluster-level**: CPU, memory, disk per node
- **Pod-level**: CPU, memory per pod
- **Application metrics**: Request rate, error rate, latency
- **Custom metrics**: Business metrics

**Viktiga dashboards:**

- Cluster overview
- Resource usage per namespace
- Pod status och restarts
- Application performance

### Logging - EFK Stack

**EFK** = Elasticsearch + Fluentd + Kibana

```
Pods (loggar) 
    â†“
Fluentd (samlar logs frÃ¥n alla pods)
    â†“
Elasticsearch (lagrar och indexerar)
    â†“
Kibana (sÃ¶ker och visualiserar)
```

**FÃ¶rdelar:**

- Centraliserad logging
- SÃ¶k Ã¶ver alla pods samtidigt
- Loggarna Ã¶verlever Ã¤ven om pods dÃ¶r

**Exempel queries:**

- "Visa alla ERROR logs senaste timmen"
- "Hitta logs frÃ¥n user-service i production namespace"
- "Vilka 500 errors har vi haft?"

### VarfÃ¶r Detta Ã„r Viktigt

Utan monitoring och logging:

- Du vet inte vad som hÃ¤nder i ditt cluster
- Du kan inte debugga problem
- Du ser inte performance issues
- Du Ã¤r blind!

Med monitoring och logging:

- Proaktiv Ã¶vervakning
- Snabb felsÃ¶kning
- Performance optimization
- Alerts nÃ¤r nÃ¥got Ã¤r fel

**I produktion: MÃ…STE HAVES!**

## Deployment Strategies

Kubernetes stÃ¶djer flera deployment-strategier.

### Rolling Update (Default)

Vi har redan pratat om detta - gradvis ersÃ¤ttning av pods.

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
```

**NÃ¤r:** Standard, fungerar fÃ¶r de flesta use cases.

### Recreate

Stoppa alla gamla pods, starta nya.

```yaml
strategy:
  type: Recreate
```

**Process:**

```
[v1] [v1] [v1]
    â†“
[ ] [ ] [ ]  â† DOWNTIME
    â†“
[v2] [v2] [v2]
```

**NÃ¤r:** NÃ¤r du inte kan ha bÃ¥de v1 och v2 samtidigt (databas-migrations, etc.).

### Blue-Green Deployment

KÃ¶r bÃ¥da versionerna, switcha trafik nÃ¤r redo.

```
1. Deploy v2 (green) parallellt med v1 (blue)
   Blue: [v1] [v1] [v1]  â† FÃ¥r trafik
   Green: [v2] [v2] [v2] â† Inget trafik

2. Testa green

3. Switch trafik frÃ¥n blue till green
   Blue: [v1] [v1] [v1]  â† Inget trafik
   Green: [v2] [v2] [v2] â† FÃ¥r trafik

4. Om problem: switch tillbaka direkt!

5. Om OK: ta bort blue
```

**Implementering i K8s:**

- TvÃ¥ Deployments: myapp-blue, myapp-green
- Service selector bytts fÃ¶r att switcha trafik

**NÃ¤r:** BehÃ¶ver snabb rollback, zero downtime kritiskt.

### Canary Deployment

Rulla ut till en liten % anvÃ¤ndare fÃ¶rst.

```
1. 100% trafik till v1
   [v1] [v1] [v1] [v1] [v1]

2. Deploy en v2 pod (10% av pods)
   [v1] [v1] [v1] [v1] [v2]
   
3. Service load balancerar:
   90% requests â†’ v1
   10% requests â†’ v2

4. Monitora metrics frÃ¥n v2

5. Om OK: Ã¶ka till 50%
   [v1] [v1] [v2] [v2]

6. Om fortfarande OK: 100%
   [v2] [v2] [v2] [v2]
```

**NÃ¤r:** High-risk deployments, behÃ¶ver validera med verklig trafik.

## Konceptuellt Exempel - E-handel pÃ¥ Kubernetes

LÃ¥t oss sÃ¤tta ihop allt vi lÃ¤rt oss med ett verkligt exempel.

### Scenario

E-handelsplattform med microservices:

- Frontend (React SPA)
- API Gateway
- User Service
- Product Service
- Order Service
- Payment Service
- Notification Service
- PostgreSQL databaser
- Redis cache

### Kubernetes Resources

**1. Frontend Deployment**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - name: frontend
          image: myregistry.com/frontend:1.2.0
          ports:
            - containerPort: 80
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
```

**2. User Service Deployment + Service**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
    spec:
      containers:
        - name: user-service
          image: myregistry.com/user-service:2.1.0
          ports:
            - containerPort: 8080
          env:
            - name: DATABASE_URL
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: user_db_url
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: user_db_password
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            requests:
              cpu: 200m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: production
spec:
  type: ClusterIP
  selector:
    app: user-service
  ports:
    - port: 80
      targetPort: 8080
```

**3. API Gateway (LoadBalancer)**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
spec:
  replicas: 3
  # ... template spec ...
---
apiVersion: v1
kind: Service
metadata:
  name: api-gateway
spec:
  type: LoadBalancer # Extern Ã¥tkomst
  selector:
    app: api-gateway
  ports:
    - port: 80
      targetPort: 8080
```

**4. Order Service + PostgreSQL**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  replicas: 3
  # ... template spec ...
---
apiVersion: v1
kind: Service
metadata:
  name: order-service
spec:
  type: ClusterIP
  selector:
    app: order-service
  ports:
    - port: 80
      targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-db
spec:
  replicas: 1
  selector:
    matchLabels:
      app: order-db
  template:
    metadata:
      labels:
        app: order-db
    spec:
      containers:
        - name: postgres
          image: postgres:15
          env:
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: order_db_password
          volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
      volumes:
        - name: postgres-storage
          persistentVolumeClaim:
            claimName: order-db-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: order-db-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: v1
kind: Service
metadata:
  name: order-db
spec:
  type: ClusterIP
  selector:
    app: order-db
  ports:
    - port: 5432
```

**5. Redis Cache**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:7
          ports:
            - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  type: ClusterIP
  selector:
    app: redis
  ports:
    - port: 6379
```

**6. ConfigMap**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  user_db_url: "postgres://order-db:5432/userdb"
  product_db_url: "postgres://product-db:5432/productdb"
  order_db_url: "postgres://order-db:5432/orderdb"
  redis_url: "redis://redis:6379"
  payment_api_url: "https://api.stripe.com"
```

**7. Secrets**

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secrets
  namespace: production
type: Opaque
data:
  user_db_password: c3VwZXJzZWNyZXQxMjM=
  product_db_password: cGFzc3dvcmQxMjM=
  order_db_password: b3JkZXJwYXNzMTIz
  stripe_api_key: c2tfdGVzdF94eHh4eHg=
```

**8. HPA fÃ¶r Auto-Scaling**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

**9. Ingress fÃ¶r Routing**

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  namespace: production
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  tls:
    - hosts:
        - myshop.com
      secretName: tls-secret
  rules:
    - host: myshop.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: frontend
                port:
                  number: 80
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api-gateway
                port:
                  number: 80
```

### Arkitekturdiagram

```
               INTERNET
                  â”‚
                  â†“
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚INGRESS â”‚ (myshop.com)
             â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚FRONTEND â”‚            â”‚   API    â”‚
â”‚ (3 rep) â”‚            â”‚ GATEWAY  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ (3 rep)  â”‚
                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â†“                    â†“                â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  USER  â”‚          â”‚PRODUCT  â”‚     â”‚ ORDER   â”‚
  â”‚SERVICE â”‚          â”‚SERVICE  â”‚     â”‚SERVICE  â”‚
  â”‚(3-20)  â”‚â†HPA      â”‚(5 rep)  â”‚     â”‚(3 rep)  â”‚
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚                    â”‚               â”‚
      â†“                    â†“               â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚USER DB â”‚          â”‚PROD DB  â”‚     â”‚ORDER DB â”‚
 â”‚Postgresâ”‚          â”‚Postgres â”‚     â”‚Postgres â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                           â”‚
                                     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                                     â”‚   PVC    â”‚
                                     â”‚  20GB    â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚         REDIS CACHE           â”‚
      â”‚          (shared)             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hur Det Fungerar

**1. Request Flow:**

```
User â†’ myshop.com 
     â†’ Ingress 
     â†’ Frontend Service 
     â†’ Frontend Pod
     
User â†’ myshop.com/api/users
     â†’ Ingress
     â†’ API Gateway Service
     â†’ API Gateway Pod
     â†’ User Service
     â†’ User Service Pod
     â†’ User DB
```

**2. Auto-Scaling:**

```
Normal dag:
- User Service: 3 pods
- CPU usage: 40%

Black Friday:
- Trafik spike!
- CPU usage: 85%
- HPA triggar
- User Service: 3 â†’ 6 â†’ 10 â†’ 15 pods
- CPU usage: 65%

Efter Black Friday:
- Trafik normaliseras
- CPU usage: 30%
- HPA skalar ner
- User Service: 15 â†’ 10 â†’ 5 â†’ 3 pods
```

**3. Self-Healing:**

```
Order Service Pod 2 kraschar:
- Deployment mÃ¤rker: "Jag ska ha 3 men har 2!"
- Startar ny pod
- Readiness probe vÃ¤ntar tills pod Ã¤r ready
- Service bÃ¶rjar skicka trafik till nya poden
- Ingen downtime - andra 2 pods hanterade trafiken
```

**4. Rolling Update:**

```
Deploy User Service v2.2.0:
- kubectl apply -f user-service-deployment.yaml
- Rolling update startar:
  [v2.1] [v2.1] [v2.1] [v2.1] [v2.1]
       â†“
  [v2.1] [v2.1] [v2.1] [v2.1] [v2.2]
       â†“
  [v2.1] [v2.1] [v2.1] [v2.2] [v2.2]
       â†“
  ... gradvis ...
       â†“
  [v2.2] [v2.2] [v2.2] [v2.2] [v2.2]
- Zero downtime!
```

**Detta Ã¤r Kubernetes i praktiken!**

## NÃ¤sta Steg fÃ¶r LÃ¤rande

Det hÃ¤r dokumentet har gett dig konceptuell fÃ¶rstÃ¥else fÃ¶r Kubernetes. FÃ¶r att lÃ¤ra dig pÃ¥ riktigt behÃ¶ver du:

### 1. Hands-On Practice

**Installera lokalt:**

```bash
# Minikube
minikube start

# Eller Docker Desktop
# Enable Kubernetes i settings
```

**GrundlÃ¤ggande Ã¶vningar:**

1. Deploy en enkel app (nginx)
2. Skapa en Service
3. Skala upp och ner
4. GÃ¶r en rolling update
5. Kolla logs
6. Exec into pods

### 2. Officiella Tutorials

**Kubernetes official docs:**

- [kubernetes.io/docs/tutorials](https://kubernetes.io/docs/tutorials/)
- Interactive browser-based tutorials
- Katacoda Kubernetes scenarios

### 3. Bygg Riktiga Projekt

**BÃ¶rja smÃ¥tt:**

- Deploy en Spring Boot app
- LÃ¤gg till databas
- LÃ¤gg till Redis
- SÃ¤tt upp Ingress
- Implementera ConfigMaps/Secrets

**Bygg stÃ¶rre:**

- Microservices-arkitektur
- Auto-scaling
- Monitoring med Prometheus/Grafana
- Logging med EFK

### 4. LÃ¤r Dig kubectl

Bli bekvÃ¤m med kubectl - det Ã¤r ditt viktigaste verktyg.

```bash
# Ã–va pÃ¥:
kubectl get, describe, logs, exec
kubectl apply, delete
kubectl scale
kubectl rollout
```

### 5. FÃ¶rstÃ¥ Networking

Kubernetes networking Ã¤r komplext:

- Pod networking
- Services och kube-proxy
- Ingress
- Network policies
- DNS

Ta tid att fÃ¶rstÃ¥ hur det fungerar.

### 6. Certifieringar (Optional)

Om du vill jobba med K8s professionellt:

**CKAD (Certified Kubernetes Application Developer)**

- Fokus: Att deploya och hantera apps
- Praktiskt exam
- Bra fÃ¶r developers

**CKA (Certified Kubernetes Administrator)**

- Fokus: Cluster administration
- Mer infrastruktur-fokuserat
- Bra fÃ¶r DevOps/SRE

### 7. Community och Resurser

- **Kubernetes Slack** - community support
- **Reddit r/kubernetes** - diskussioner och frÃ¥gor
- **YouTube** - tutorials och talks
- **Blogs** - mÃ¥nga bra K8s blogs
- **Books** - "Kubernetes in Action", "Kubernetes Up & Running"

## Sammanfattning

**Kubernetes** Ã¤r container orchestration-plattformen som gÃ¶r det mÃ¶jligt att kÃ¶ra containers i produktion pÃ¥ stor skala.

### Nyckelpunkter:

**Vad Ã¤r K8s:**

- Container orchestration system
- Multi-host clustering
- Open source, industry standard
- Ursprung: Google Borg

**VarfÃ¶r K8s:**

- Auto-scaling (bÃ¥de pods och nodes)
- Self-healing (pods startas automatiskt om)
- Rolling updates (zero downtime deployments)
- Service discovery (inbyggd)
- Load balancing (inbyggd)
- Multi-server management
- Declarative configuration

**Grundkoncept:**

- **Pod** - minsta deployable unit (1+ containers)
- **Deployment** - hanterar pods, rolling updates
- **Service** - stabil IP/DNS fÃ¶r pods, load balancing
- **Namespace** - virtuella sub-clusters
- **ConfigMap/Secret** - konfiguration och kÃ¤nslig data
- **PersistentVolume** - storage som Ã¶verlever pods
- **Labels/Selectors** - organisera och hitta resurser

**Arkitektur:**

- **Control Plane** - API Server, Scheduler, Controller Manager, etcd
- **Worker Nodes** - Kubelet, Container Runtime, Kube-proxy
- **kubectl** - CLI fÃ¶r att interagera med cluster

**K8s Features:**

- Declarative state management
- Continuous monitoring och correction
- Health checks (liveness, readiness, startup)
- Resource management (requests/limits)
- Auto-scaling (HPA, Cluster Autoscaler)
- Multiple deployment strategies

**Docker Compose vs K8s:**

- Compose: Single host, enkel, perfekt fÃ¶r dev/test
- K8s: Multi-host, komplex, perfekt fÃ¶r produktion

**Managed Services:**

- GKE (Google)
- EKS (AWS)
- AKS (Azure)
- â†’ LÃ¤ttare Ã¤n self-hosted

**NÃ¤r anvÃ¤nda K8s:**
âœ“ Microservices i stor skala
âœ“ Flera team/services
âœ“ High availability krÃ¤vs
âœ“ Auto-scaling behÃ¶vs
âœ— Liten app/team
âœ— Docker Compose rÃ¤cker
âœ— Ingen K8s-expertis

**Complexity Cost:**

- Learning curve
- Operational overhead
- Networking complexity
- Monitoring/logging setup
- â†’ VÃ¤g fÃ¶rdelar mot kostnader

**K8s + Microservices:**

- Perfekt kombination
- Oberoende deployment och scaling
- Built-in service discovery
- Detta Ã¤r dÃ¤r K8s shinar!

**NÃ¤sta steg:**

- Hands-on practice (Minikube, Docker Desktop)
- Officiella tutorials
- Bygg riktiga projekt
- LÃ¤r kubectl
- Ã–vervÃ¤g certifiering (CKAD, CKA)

### Vad Kommer HÃ¤rnÃ¤st?

Nu nÃ¤r du fÃ¶rstÃ¥r Kubernetes konceptuellt Ã¤r nÃ¤sta steg **CI/CD pipelines**. Hur deployer du automatiskt till Kubernetes nÃ¤r du pushar kod till Git? Det Ã¤r dÃ¤r continuous integration och continuous deployment kommer in!

## Ã–vningsuppgifter

**1. Konceptuell FÃ¶rstÃ¥else**

FÃ¶rklara relationen mellan Pod, ReplicaSet och Deployment. VarfÃ¶r har Kubernetes denna hierarki istÃ¤llet fÃ¶r att bara ha "Pods med replicas"?

**2. Self-Healing Scenario**

Du har en Deployment med 5 replicas. Beskriv steg-fÃ¶r-steg vad som hÃ¤nder i Kubernetes nÃ¤r:
a) En pod kraschar
b) En hel worker node gÃ¥r ner (med 2 pods pÃ¥ den)

Vilka Kubernetes-komponenter Ã¤r involverade i varje steg?

**3. Arkitekturdiagram**

Rita ett diagram Ã¶ver Kubernetes cluster-arkitekturen. Inkludera:

- Control Plane (och dess komponenter)
- Worker Nodes (och dess komponenter)
- Hur de kommunicerar
- Var pods kÃ¶rs

**4. Docker Compose vs Kubernetes**

Du har en applikation som bestÃ¥r av:

- En Node.js API (Express)
- En React frontend
- En PostgreSQL databas
- En Redis cache

Just nu kÃ¶r du pÃ¥ en server med Docker Compose.

a) NÃ¤r skulle du Ã¶vervÃ¤ga att migrera till Kubernetes?
b) Lista minst 5 konkreta tecken/triggers som indikerar att det Ã¤r dags
c) Vilka Ã¤r riskerna med att migrera fÃ¶r tidigt?

**5. E-handel pÃ¥ Kubernetes**

Design en microservices e-handelsplattform pÃ¥ Kubernetes. Din design ska inkludera:

**Services:**

- Frontend (React)
- API Gateway
- Authentication Service
- Product Service
- Cart Service
- Order Service
- Payment Service
- Email Service

**FÃ¶r varje service, specificera:**
a) Antal replicas (med motivering)
b) Resource requests/limits (rough estimates)
c) Vilka Services behÃ¶ver vara exponerade externt vs internt
d) Vilka services behÃ¶ver persistent storage
e) Vilka services skulle du auto-scala (HPA)

**Rita ocksÃ¥:**

- Arkitekturdiagram med alla services
- Request flow frÃ¥n anvÃ¤ndare till backend
- Vilka Kubernetes resources behÃ¶vs (Deployments, Services, PVCs, etc.)

**6. Scaling Analys**

Din applikation har fÃ¶ljande behavior:

- Normal dag: 1000 requests/sekund, varje pod klarar 200 req/s
- Lunch rush (12-13): 3000 requests/sekund
- KvÃ¤ll (18-20): 5000 requests/sekund
- Natt (00-06): 200 requests/sekund

a) Designa en HPA-konfiguration fÃ¶r detta. Hur mÃ¥nga min/max replicas?
b) BerÃ¤kna ungefÃ¤r hur mÃ¥nga pods som kommer kÃ¶ra vid olika tider
c) FÃ¶rklara skillnaden mellan manual scaling och auto-scaling fÃ¶r detta use case
d) Vilka metrics skulle du Ã¶vervaka (fÃ¶rutom CPU)?

**7. Service Discovery**

FÃ¶rklara hur Service discovery fungerar i Kubernetes:

a) Du har en `order-service` som behÃ¶ver prata med `user-service`. Hur hittar order-service user-service utan hardcoded IPs?
b) Vad hÃ¤nder nÃ¤r user-service pods fÃ¥r nya IP-adresser efter en restart?
c) Hur implementeras load balancing mellan de 5 user-service pods automatiskt?
d) Skriv ett exempel pÃ¥ HTTP-anrop frÃ¥n order-service till user-service med rÃ¤tt URL

---

**Bonus: Hands-On (Om du har miljÃ¶ setup)**

8. Installera Minikube eller Docker Desktop K8s och:
   - Deploy nginx med 3 replicas
   - Skapa en Service fÃ¶r att exponera den
   - Skala upp till 5 replicas
   - GÃ¶r en rolling update till annan nginx-version
   - Dokumentera varje kommando och output

Lycka till! Kubernetes Ã¤r komplext men otroligt kraftfullt nÃ¤r du vÃ¤l fÃ¶rstÃ¥r det. ğŸš€
